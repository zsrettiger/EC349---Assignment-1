---
title: <div align="center">"EC349 - Assignment 1"
date: <div align="center"> "2023-11-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<div align="center">
## Prediction of the star rating of users on Yelp
*University ID: u2080302*


Link to my GitHub Repository: https://github.com/zsrettiger/EC349---Assignment-1

```{r, include=FALSE}
#Clear
cat("\014") 
rm(list=ls())

#Set Directory 
setwd("C:/Users/zsret/Documents/University of Warwick - Year 4/EC349 - Data Science/EC349 - Assignment 1")

#Load Libraries
library(glmnet)
library(ggplot2)
library(tidyverse)
library(tree)
library(rpart)
library(rpart.plot)
library(ipred) 
library(caret)  
library(randomForest) 

#Load Datasets 
load(file='yelp_review_small.Rda')
load(file='yelp_user_small.Rda')
user_review <- merge(review_data_small, user_data_small,by = "user_id")

#Create Test and Training Data sets
set.seed(1)
parts <- createDataPartition(user_review$stars, times = 1,  p = 10000/nrow(user_review), list = F)
#Exclude variables that are characters, for the rest of the variables: stars and average_stars are numeric and the rest are integers
test <- user_review[parts,-c(1,2,3,8,9,10,12,16,17)]
test_x <-test[,-1] 
test_y <-test[,1]
train <- user_review[-parts, -c(1,2,3,8,9,10,12,16,17)]
train_x <-train[,-1]
train_y <-train[,1]
```

<div align="left">

**Tabula statement**

We're part of an academic community at Warwick.

Whether studying, teaching, or researching, we’re all taking part in an expert conversation which must meet standards of academic integrity. When we all meet these standards, we can take pride in our own academic achievements, as individuals and as an academic community.

Academic integrity means committing to honesty in academic work, giving credit where we've used others' ideas and being proud of our own achievements.

In submitting my work I confirm that:

1. I have read the guidance on academic integrity provided in the Student Handbook and understand the University regulations in relation to Academic Integrity. I am aware of the potential consequences of Academic Misconduct.

2. I declare that the work is all my own, except where I have stated otherwise.

3. No substantial part(s) of the work submitted here has also been submitted by me in other credit bearing assessments courses of study (other than in certain cases of a resubmission of a piece of work), and I acknowledge that if this has been done this may lead to an appropriate sanction.

4. Where a generative Artificial Intelligence such as ChatGPT has been used I confirm I have abided by both the University guidance and specific requirements as set out in the Student Handbook and the Assessment brief. I have clearly acknowledged the use of any generative Artificial Intelligence in my submission, my reasoning for using it and which generative AI (or AIs) I have used. Except where indicated the work is otherwise entirely my own.

5. I understand that should this piece of work raise concerns requiring investigation in relation to any of points above, it is possible that other work I have submitted for assessment will be checked, even if marks (provisional or confirmed) have been published.

6. Where a proof-reader, paid or unpaid was used, I confirm that the proofreader was made aware of and has complied with the University’s proofreading policy.

7. I consent that my work may be submitted to Turnitin or other analytical technology. I understand the use of this service (or similar), along with other methods of maintaining the integrity of the academic process, will help the University uphold academic standards and assessment fairness.

Privacy statement

The data on this form relates to your submission of coursework. The date and time of your submission, your identity, and the work you have submitted will be stored. We will only use this data to administer and record your coursework submission.

Related articles

Reg. 11 Academic Integrity (from 4 Oct 2021)
Guidance on Regulation 11
Proofreading Policy  
Education Policy and Quality Team
Academic Integrity (warwick.ac.uk)


### Questions

***Question 3.: You will write up an analysis of your chosen method and of your results, in 1250 words.***


  In this analysis, I use 20 numeric and integer variables from data sets about user characteristics and reviews merged together, containing 289878 observations, to predict the number of stars given by user i to business j.  

  When predicting the amount of stars given by user i, both the outcome of interest - Y, that is, the number of stars given, and the characteristics - X - of users and reviews can be observed, thus, this is a case of supervised learning. Additionally, in the case of this prediction, the outcome variable is categorical, taking on the values of 1,2,3,4 and 5, such that it requires the use of classification methods. Consequentially, the use of decision tree based models, such as categorical decision trees, bagged decision trees and random forest, could lead to the most precise predictions. 
  
  Firstly, I look at categorical decision trees, which are non-parametric supervised-learning methods, that split the feature space of X characteristics into regions based on informativeness, such that the variable best classifying the data is split first (IBM, 2022). In this case, as shown on the decision tree below, the parent node is a five-star review, while the child notes are one- and five-star reviews. Two-, three- and four-star reviews remain unused in the decision tree, suggesting that they did not contribute to the change in entropy in any of the splits that could lead to a gain in prediction.  According to the tree on the training data, average stars given by users is the most important variable when predicting the stars a user will give, with review count, a cool and a useful rating also being significant variables in the prediction. Receiving a review from a user giving less than three stars on average results in a prediction of one star based on these results.     

<div align="center">
```{r Question 3: Decision Tree, echo=FALSE }
#Classification Tree
rpart_treec<-rpart(stars ~ ., data = train, method = 'class')
rpart.plot(rpart_treec)
```
   
   
  <div align="left">
  To see how accurate these results are, I create an accuracy measure by comparing the number of stars correctly specified by the model in the test set with the observed stars. I use this measure as I cannot use other measures, such as the mean squared error, because the outcome is categorical. The constructed accuracy measure suggests that the categorical decision tree predicts the stars given by users with a 51.83 percent accuracy in the test set. Comparing these results with the prediction of the regression tree, the regression tree has an accuracy of 35.5 percent in the test set when rounding up to the number of stars to integers, confirming that the categorical tree is a better fit for this data. 
  
```{r Question 3: Decision Tree accuracy, echo=FALSE}
rpart_treec<-rpart(stars ~ ., data = train, method = 'class')
rpart_predictionc <- predict(rpart_treec, newdata = test, type="class")
accuracy_rpartc <- sum(rpart_predictionc == test_y) / nrow(test)
cat('Accuracy for Classification Tree (rpart):', accuracy_rpartc, '\n')
    
#Regression Tree
rpart_treer<-rpart(stars ~ ., data = train, method = 'anova')
rpart_predictionr <- predict(rpart_treer, newdata = test)
accuracy_rpartr <- sum(round(rpart_predictionr) == test_y) / nrow(test)
cat('Accuracy for Regression Tree (rpart):', accuracy_rpartr, '\n')
rpart_MSEr<- mean((rpart_predictionr - test_y) ^ 2) 
cat('MSE for Regression Tree (rpart):', rpart_MSEr, '\n')
```
  
  
  Decision trees have the advantage of being flexible and easy to interpret. However, the prediction results have a high variance. To account for this, I re-estimate the predictions using the method of bagging regression trees, which decrease the variance by averaging over observations. The bagging model has an out-of-bag estimate of root mean squared error of 1.3092 and a mean squared error of 1.7189 in the training set. The accuracy of the bagging model is 22.16 percent in the test set; thus, it does not outperform the decision tree on this data set. However, it is important to note that the accuracy of the prediction is measured by rounding up the number of stars  to integers, such that this result is not completely accurate. 
  
```{r Question 3: Bagging accuracy, echo=FALSE}
set.seed(123)     
bag <- bagging(stars ~., data = train, nbagg = 20,   
                 coob = TRUE, control = rpart.control(minsplit = 2, cp = 0.1))

bag_predictions <- predict(bag, newdata = test)
bag_MSE<- mean((bag_predictions - test_y) ^ 2) 
cat('MSE for Bagging:', bag_MSE, '\n')
accuracy_bag <- sum(round(bag_predictions) == test_y) / nrow(test)
cat('Accuracy for Bagging:', accuracy_bag, '\n')
```
  
  I also estimate the predictions using the Random Forest model. Random Forests combine random trees and bagging, such that they remove correlation in the predictions, which can occur in decision trees because the predictions were generated from the same original data set, while also reducing the variance in the prediction in comparison to decision trees. The Random Forest model has an out-of-bag estimate of  error rate of 46 percent on the training data, and predicts the stars given by users with an accuracy of 99.95 percent, and an error rate of 0.03024 on the test set, outperforming the categorical decision tree and the bagged regression tree. Additionally, looking at the mean decrease in Gini coefficient, which measures the contribution of the variables to the homogeneity of the nodes and leaves of the random forest tree (Martinez-Taboada and Redondo, 2020), average stars given by users is the most important variable in predicting the number of stars business j will receive, followed by the variables review count, useful and cool, aligning with the results provided by the categorical decision tree. 
  

```{r Question 3: Random Forest accuracy, echo=FALSE}
stars_f <- as.factor(train$stars)
set.seed(12)
model_RF<-randomForest(stars_f ~.,data=train, ntree=20)
pred_RF_test = predict(model_RF, test)
RF_err <- mean(model_RF[["err.rate"]])
cat('Error rate for Random Forest:', RF_err, '\n')

accuracy_rf <- sum(pred_RF_test == test_y) / nrow(test)
cat('Accuracy for Random Forest:', accuracy_rf, '\n')
importance(model_RF)
```
  
  Additionally, I estimate the predictions using linear regression models and shrinkage methods to compare to their results to the decision tree based models, treating the outcome variable as a continuous variable and rounding it up to integers when calculating the accuracy of the models. For the Linear Regression model, MSE of 1.445429 is observed, for Ridge, the MSE equals to 1.409148 and for LASSO, the MSE is 1.412232 on the test set. Thus, the Ridge model is suggested to perform the best in predicting the outcome variable. In terms of accuracy on the test set, the Linear Regression model is 37 percent accurate in predicting the outcome, while Ridge is 36.34 percent and LASSO is 37.38 percent accurate, suggesting that the LASSO model is best for prediction of stars given by customers. However, the accuracy scores of these models are much lower than that of the categorical decision tree and the random forest, confirming the choice of models.  
  
```{r Question 3: Linear, echo=FALSE}
##Linear Regression
lm_stars <- lm(stars ~ average_stars + funny.x + compliment_cool + 
                 compliment_cute + compliment_more + compliment_note +
                 compliment_photos + review_count, data = train) 
lm_stars_predict<-predict(lm_stars, newdata = test)
lm_stars_test_MSE<-mean((lm_stars_predict-test$stars)^2)
cat('MSE for OLS:', lm_stars_test_MSE, '\n')
accuracy_lm <- sum(round(lm_stars_predict) == test_y) / nrow(test)
cat('Accuracy for OLS:', accuracy_lm, '\n')

##Ridge with Cross-Validation 
cv.out <- cv.glmnet(as.matrix(train_x), as.matrix(train_y), alpha = 0)
lambda_ridge_cv <- cv.out$lambda.min 
ridge.mod<-glmnet(train_x, train_y, alpha = 0, lambda = lambda_ridge_cv, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = lambda_ridge_cv, newx = as.matrix(test_x))
ridge_MSE<- mean((ridge.pred - test_y) ^ 2) 
cat('MSE for RIDGE:', ridge_MSE, '\n')
accuracy_ridge <- sum(round(ridge.pred) == test_y) / nrow(test)
cat('Accuracy for RIDGE:', accuracy_ridge, '\n')

##LASSO with Cross-Validation 
cv.out <- cv.glmnet(as.matrix(train_x), as.matrix(train_y), alpha = 1, nfolds = 3)
lambda_LASSO_cv <- cv.out$lambda.min  
LASSO.mod<-glmnet(train_x, train_y, alpha = 1, lambda = lambda_LASSO_cv, thresh = 1e-12)
LASSO.pred <- predict(LASSO.mod, s = lambda_LASSO_cv, newx = as.matrix(test_x))
LASSO_MSE <- mean((LASSO.pred - test_y) ^ 2) #Note how it outperforms OLS
cat('MSE for LASSO:', LASSO_MSE, '\n')
accuracy_lasso <- sum(round(LASSO.pred) == test_y) / nrow(test)
cat('Accuracy for LASSO:', accuracy_lasso, '\n')
```
  
  
  Following these results, the most accurate method to predict the number of stars given to business j by user i based on the characteristics of the users and their reviews is the Random Forest model with an accuracy of 99.95 percent. It is important to consider that I have used the small data sets with less observations and variables, such that these results might not be accurate. Moreover, I have excluded variables containing text, ignoring the importance of these variables, which can further decrease the accuracy of my results. 
  
<div align="right">
(972 words)

<div align="left">
***A brief description of your chosen DS methodology, why you chose this and how you applied it:***

Data Science Methodologies provide a system of approaches to achieve efficient and repeatable results. In this project I used the Cross-Industry Standard Process for Data Mining - CRISP-DM, as I have found its six stages to allow for an easier iterative process. In the 'Data Understanding' stage, I collected the data sets, then explored the details of the data, so that I can work with it in the next steps. For the 'Data Preparation' process, I merged my data sets, and reformatted and excluded variables so that I can work with them while modelling. In the 'Modelling' step, I selected, constructed and assessed my chosen models for the predictions on both the test and training data sets. Then I evaluated my results, followed by a final report about the results in the form of this analysis as part of the "Deployment'. 

<div align="right">
(141 words)

<div align="left">
***A statement on your most difficult challenge (if any) carrying out this project and how you overcame/handled it:***

<div align="right">
( words)
(Overall: words)



<div align="left">
**References:**

IBM (2017) 'IBM SPSS Modeler CRISP-DM Guide', [online] www.ibm.com. Available at: https://www.ibm.com/docs/en/spss-modeler/18.1.1?topic=spss-modeler-crisp-dm-guide.

IBM (2022) 'What is a Decision Tree' [online] www.ibm.com. Available at: https://www.ibm.com/topics/decision-trees.

Luo, K., Li, M., Xia, S. and Lin, Z. (n.d.) Prediction of Yelp Star Rating. [online] Available at: https://cseweb.ucsd.edu/classes/wi15/cse255-a/reports/fa15/015.pdf.

Martinez-Taboada, F. and Redondo, J. I. (2020) 'Variable importance plot (mean decrease accuracy and mean decrease Gini)', *PLOS ONE*. Figure. https://doi.org/10.1371/journal.pone.0230799.g002.

